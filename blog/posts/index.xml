<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on The Evening Paper</title>
    <link>https://millyz.github.io/blog/posts/</link>
    <description>Recent content in Posts on The Evening Paper</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Mon, 03 Dec 2018 17:21:30 +0800</lastBuildDate>
    <atom:link href="https://millyz.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Overload Control for Scaling WeChat Microservices, SoCC &#39;18</title>
      <link>https://millyz.github.io/blog/posts/dagor-socc18/</link>
      <pubDate>Mon, 03 Dec 2018 17:21:30 +0800</pubDate>
      
      <guid>https://millyz.github.io/blog/posts/dagor-socc18/</guid>
      <description>This paper presents the microservice architecture and overload control at WeChat. We can take a look how such a massive application runs internally, which is the point attracting me most. This paper is easy to follow and understand. In conclusion, the technique introduced in this paper seems straightforward and practical.
The motivation of this paper is quite strong and very clear. &amp;ldquo;Workload handled by the WeChat backend is always varying over time, and the fluctuation pattern differs among diverse situations&amp;rdquo;.</description>
      <content type="html"><![CDATA[<p>This paper presents the microservice architecture and overload control at
WeChat. We can take a look how such a massive application runs internally,
which is the point attracting me most. This paper is easy to follow and
understand. In conclusion, the technique introduced in this paper seems
straightforward and practical.</p>

<p>The motivation of this paper is quite strong and very clear. &ldquo;Workload handled
by the WeChat backend is always varying over time, and the fluctuation pattern
differs among diverse situations&rdquo;. The request amount during peak hours is
about 3 times larger than the daily average, even 10 times during the period of
Chinese Lunar New Year. Obviously, it is not economic to run physical machines
based on the maximum request amount. Therefore, overload control mechanism is
required to adaptively tolerate the workload fluctuation at system runtime.</p>

<p>This paper targets to solve <em>subsequent overload</em>, where more than one
overloaded services or the single overloaded service is invoked multiple times
by the associated upstream services. In this scenario, the inappropriate load
shedding can aggravate the overload situation. Basically, the upstream service
re-sends requests to a overloaded server and a larger fraction of requests
fails, which degrade the overall throughput. Therefore, simple load shedding
does not work here.  The authors need to tackle with the following challenges
when designing overload control algorithm for WeChat:</p>

<ul>
<li>No single entry point for service requests sent to the Wechat backend.

<ul>
<li>Centralized load monitoring at a global entry point is not valid.</li>
<li>Fail to pinpoint the slow service due to the complex call path.</li>
</ul></li>
<li>Excessive request abortion not only wastes the computational resources but
also increases the latency of service response.

<ul>
<li>Call for proper management on load shedding.</li>
</ul></li>
</ul>

<p>The proposed overload control scheme for WeChat is called DAGOR, consisting
<strong>overload detection</strong> and <strong>service admission control</strong>.</p>

<p>DAGOR takes the <em>queuing time</em> to profile the load status of a server. The
queuing time is measured by the time difference between the request arrival and
its processing being started at the server. When a server becomes overloaded
due to resource exhaustion, the queuing time rises proportional to the excess
workload. They do not use the response time because it additionally counts the
request processing time, which may be inaccurate to reflect the load status.</p>

<p>For service admission control, DAGOR mainly considers <em>business-oriented
admission control</em>, <em>user-oriented admission control</em>, <em>adaptive admission
control</em> and <em>collaborative admission control</em>:</p>

<ul>
<li>Business-oriented admission control, the request with higher business
significance and greater impact on user experience is of the higher priority.
For example, payment service has higher priority than messaging while
messaging is more important than moments. The business priorities are
predefined and stored in a hash table, which remains stable.</li>
<li>User-oriented admission control, discard the requests of some user which has
lower user priority at the same business priority. The user priority is
generated by a hash function using the user ID and changes every hour for
fairness. They also mention session-oriented admission control, which is
similar to user-oriented admission control.</li>
<li>Adaptive admission control, reject more incoming requests restrictedly and
relax the admission control for mild overload situation. DAGOR combines the
above two schemes with respect to the individual load status. Based on the
result of load detection, the server increases/decreases the expected amount of
incoming requests. Then the sever can set a proper business and user priority.</li>
<li>Collaborative admission control, the downstream server sends its admission level to the upstream server such that some requests can be rejected by the upstream server early.</li>
</ul>

<p>DAROR has been deployed in Wechat for more than five year, demonstrating its effectiveness and robustness. A practical design indeed.</p>
]]></content>
    </item>
    
    <item>
      <title>Clay codes: Moulding MDS Codes to Yield an MSR Code, FAST &#39;18</title>
      <link>https://millyz.github.io/blog/posts/claycodes-fast18/</link>
      <pubDate>Mon, 29 Oct 2018 19:00:00 +0800</pubDate>
      
      <guid>https://millyz.github.io/blog/posts/claycodes-fast18/</guid>
      <description>The title attracts me at the first sight. How do the authors switch a MDS code to a MSR code though they belong to different classes of erasure codes from my understanding? After reading the abstract, a big question mark arises over my head as the properties of this code seems rather promising. This new code, termed as Clay codes (short for Coupled-Layer), achieves the following properties:
 Low storage overhead; Optimal in repair bandwidth, sub-packetization level and disk I/O; Uniform repair performance of data and parity nodes; Support for both single and multiple-node repairs, while permitting faster and more efficient repair.</description>
      <content type="html"><![CDATA[

<p>The title attracts me at the first sight. How do the authors switch a MDS code to
a MSR code though they belong to different classes of erasure codes from my
understanding? After reading the abstract, a big question mark arises over my
head as the properties of this code seems rather promising. This new code,
termed as <em>Clay codes</em> (short for Coupled-Layer), achieves the following
properties:</p>

<ol>
<li>Low storage overhead;</li>
<li>Optimal in repair bandwidth, sub-packetization level and disk I/O;</li>
<li>Uniform repair performance of data and parity nodes;</li>
<li>Support for both single and multiple-node repairs, while permitting faster
and more efficient repair.</li>
</ol>

<p>We can look at how Clay codes offer these advantages one by one later.  Before
going into the details of Clay codes, we first review the motivation and
challenges of this work.  In my opinion, they really try to solve
a challenging and practical problem. Now more and more distributed storage
systems employ erasure coding to protect data against failures. And one class of
widely-used erasure codes is <em>Maximum Distance Separable (MDS) codes</em>, e.g.,
Reed-Solomon codes. However, MDS codes incur high repair bandwidth while they
offer minimum storage overhead. Therefore, another class of codes named
<em>Minimum Storage Regenerating (MSR) codes</em> is proposed to minimize the repair
bandwidth cost while keeping low storage overhead and tolerating faults as MDS
codes. But the authors think the current MSR codes are not practical enough to
adopt in real-world storage systems. They consider the following points:</p>

<ul>
<li>Complex computation, e.g., nccloud;</li>
<li>Non-uniform repair for different types of node failures, e.g., HashTag;</li>
<li>Tolerate limited number of failures, e.g., Butterfly;</li>
<li>Uncommon construction of erasure codes, e.g., PM-RMT, Butterfly.</li>
</ul>

<p>Well, Clay codes do beat the existing MSR codes from the following
perspectives. Actually this paper can be considered as an extension of their
previous paper. The key idea of Clay codes is &ldquo;Clay codes are constructed by
placing any MDS code in multiple layers and performing pair-wise coupling
across layers.&rdquo; No magic in computer science but math exists. They add
additional computation to encoding/decoding such that minimum data can complete
the recovery as MSR codes. During encoding, Clay codes convert to uncoupled
code via pairwise reverse transform (PRT), perform MDS encoding, then turn back
to coupled codes via pairwise forward transform (PFT). For single-node
recovery, Clay codes first perform PRT to obtain the uncoupled code, MDS
decoding, and then compute the coupled element. Briefly speaking, the less
repair bandwidth cost derives from the coupling (or spreading data across
layers), i.e., any two sub-chunks out of {U, U∗, C, C∗} can be computed from
remaining two (Q: why not one-to-one?).  Detailed explanation can be found in
the slides.</p>

<p>They implement Clay codes on Ceph, which is the first vector codes running on
Ceph. In the evaluation, they compare Clay codes with RS codes, which I think
can be better if considering other MSR codes. Hope I can implement it by myself
someday and continue to compare it with other erasure codes.</p>

<h2 id="references">References:</h2>

<ul>
<li><a href="https://www.usenix.org/system/files/conference/fast18/fast18-vajha.pdf" target="_blank">Paper</a></li>
<li><a href="https://www.usenix.org/sites/default/files/conference/protected-files/fast18_slides_vajha.pdf" target="_blank">Authors&rsquo;
slides</a></li>
</ul>
]]></content>
    </item>
    
  </channel>
</rss>
